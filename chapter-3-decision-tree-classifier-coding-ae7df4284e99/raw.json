[{
  "tag": "P",
  "text": "If you liked this post, share with your interest group, friends and colleagues. Comment down your thoughts, opinions and feedback below. I would love to hear from you. Follow machine-learning-101 for regular updates. Don’t forget to click the heart(❤) icon. You can write to me at savanpatel3@gmail.com . Peace.",
  "translation": "如果您喜欢这篇文章，请与您的兴趣小组，朋友和同事分享。 在下面写下您的想法，意见和反馈。 我希望收到您的来信。 请按照machine-learning-101进行定期更新。 别忘了点击心脏（❤）图标。 您可以通过savanpatel3@gmail.com给我写信。 和平。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*enrAyOuSgxqrnDo_ErzzTw.jpeg?q=20",
  "type": "image",
  "file": "1!enrAyOuSgxqrnDo_ErzzTw.jpeg"
}, {
  "tag": "H1",
  "text": "Entering the world of Decision Tree Classifier",
  "translation": "进入决策树分类器的世界"
}, {
  "tag": "P",
  "text": "The code for decision tree classifier is similar to previous two classifiers Naive Bayes and SVM. We import tree library. Next we extract the features and labels. We train them into model. And then predict. Later, we shall check accuracy.",
  "translation": "决策树分类器的代码与之前的两个分类器Naive Bayes和SVM相似。 我们导入树库。 接下来，我们提取特征和标签。 我们将它们训练成模型。 然后预测。 稍后，我们将检查准确性。"
}, {
  "tag": "PRE",
  "text": "from sklearn import treefrom sklearn.metrics import accuracy_scoreTRAIN_DIR = \"../train-mails\"TEST_DIR = \"../test-mails\"dictionary = make_Dictionary(TRAIN_DIR)print \"reading and processing emails from file.\"features_matrix, labels = extract_features(TRAIN_DIR)test_feature_matrix, test_labels = extract_features(TEST_DIR)model = tree.DecisionTreeClassifier()print \"Training model.\"#train modelmodel.fit(features_matrix, labels)predicted_labels = model.predict(test_feature_matrix)print \"FINISHED classifying. accuracy score : \"print accuracy_score(test_labels, predicted_labels)",
  "translation": "来自sklearn导入树，来自sklearn.metrics导入precision_scoreTRAIN_DIR =“ ../train-mails\"TEST_DIR =” ../test-mails\"dictionary = make_Dictionary（TRAIN_DIR）print“读取和处理来自文件的电子邮件。” features_matrix，标签= extract_features（ TRAIN_DIR）test_feature_matrix，test_labels = extract_features（TEST_DIR）model = tree.DecisionTreeClassifier（）print“训练模型。”＃train modelmodel.fit（features_matrix，标签）predicted_labels = model.predict（test_feature_matrix）print“ FINISHED”分类。 打印精度得分（test_labels，预测的labels）"
}, {
  "tag": "P",
  "text": "What was the accuracy score? You will have received around 91.53%."
}, {
  "tag": "H1",
  "text": "Now let’s explore some tuning parameters and try to make training faster.",
  "translation": "现在，让我们探索一些调整参数，并尝试使训练更快。"
}, {
  "tag": "H2",
  "text": "Minimum sample split",
  "translation": "最小样品分裂"
}, {
  "tag": "P",
  "text": "Ideally, decision tree stops splitting the working set based on features either it runs out of features or working set ends up in same class. We can make faster by tolerating some error at minimum split criteria. With this parameter, decision tree classifier stops the splitting if the number of items in working set decreases below specified value.",
  "translation": "理想情况下，决策树停止根据功能拆分工作集，要么是用尽了功能，要么是工作集落入同一类。 我们可以通过以最小分割标准容忍某些错误来加快速度。 使用此参数，如果工作集中的项目数减少到指定值以下，则决策树分类程序将停止拆分。"
}, {
  "tag": "P",
  "text": "Following is the diagram where Minimum Sample split is 10.",
  "translation": "下图是最小样本分割为10的图。"
}, {
  "tag": "P",
  "text": "Default in sklearn library is 2.",
  "translation": "sklearn库中的默认值为2。"
}, {
  "tag": "P",
  "text": "Try providing this parameter as 40",
  "translation": "尝试将此参数设置为40"
}, {
  "tag": "PRE",
  "text": "model = tree.DecisionTreeClassifier(min_samples_split=40)",
  "translation": "模型= tree.DecisionTreeClassifier（min_samples_split = 40）"
}, {
  "tag": "P",
  "text": "What is the accuracy here? You will have got accuracy around 87.3%."
}, {
  "tag": "H2",
  "text": "Criteria for split : criterion",
  "translation": "分割标准：标准"
}, {
  "tag": "P",
  "text": "In theory part we learned that one of good spliting decision is to take one that provides the best information gain. Criteria for sklearn can be gini or entropy( for information gain). The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.",
  "translation": "从理论上讲，我们了解到良好的拆分决策之一是采取能够提供最佳信息增益的决策。 sklearn的标准可以是gini或熵（用于获取信息）。 衡量分割质量的功能。 支持的标准是对基尼杂质的“基尼”和对信息增益的“熵”。"
}, {
  "tag": "P",
  "text": "Try these two and checkout what is accuracy.",
  "translation": "尝试这两个并检查什么是准确性。"
}, {
  "tag": "PRE",
  "text": "model = tree.DecisionTreeClassifier(criterion=\"entropy\")",
  "translation": "模型= tree.DecisionTreeClassifier（criterion =“ entropy”）"
}, {
  "tag": "P",
  "text": "and",
  "translation": "和"
}, {
  "tag": "PRE",
  "text": "model = tree.DecisionTreeClassifier(criterion=\"gini\")",
  "translation": "模型= tree.DecisionTreeClassifier（criterion =“ gini”）"
}, {
  "tag": "P",
  "text": "You can find detailed parameters here : http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier",
  "translation": "您可以在此处找到详细的参数：http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"
}, {
  "tag": "H1",
  "text": "Final Thoughts",
  "translation": "最后的想法"
}, {
  "tag": "P",
  "text": "Decision tree is classification strategy as opposed to the algorithm for classification. It takes top down approach and uses divide and conquer method to arrive at decision. We can have multiple leaf classes with this approach.",
  "translation": "决策树是分类策略，而不是分类算法。 它采用自上而下的方法，并使用分而治之的方法来做出决策。 通过这种方法，我们可以有多个叶子类。"
}, {
  "tag": "H2",
  "text": "What next",
  "translation": "接下来是什么"
}, {
  "tag": "P",
  "text": "In next part we shall discuss the k-nearest neighbours algorithm and implement a small code again using sklearn library. We shall explore tuning parameters, three different approaches for k-nearest neighbour selection.",
  "translation": "在下一部分中，我们将讨论k最近邻算法，并使用sklearn库再次实现小代码。 我们将探索调整参数，这是k近邻选择的三种不同方法。"
}, {
  "tag": "H1",
  "text": "Chapter 3 : Decision Tree Classifier — Coding",
  "translation": "第三章：决策树分类器—编码"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*r7oflh0xQVbre2dSzP2mLA.png?q=20",
  "type": "image",
  "file": "1!r7oflh0xQVbre2dSzP2mLA.png"
}, {
  "tag": "P",
  "text": "In this second part we try to explore sklearn library’s decision tree classifier. We shall tune parameters discussed in theory part and checkout accuracy results.",
  "translation": "在第二部分中，我们尝试探索sklearn库的决策树分类器。 我们将调整理论部分讨论的参数，并检验结果的准确性。"
}, {
  "tag": "P",
  "text": "While you will get fair enough idea about implementation just by reading, I strongly recommend you to open editor and code along with the tutorial. I will give you better insight and long lasting learning.",
  "translation": "通过阅读您将对实现有足够的了解，但我强烈建议您在打开教程的同时打开编辑器和代码。 我将为您提供更好的见识和长期的学习。"
}, {
  "tag": "H2",
  "text": "0. What shall we be doing.",
  "translation": "0.我们要做什么。"
}, {
  "tag": "P",
  "text": "Don’t forget to hit ❤",
  "translation": "别忘了打❤"
}, {
  "tag": "P",
  "text": "Coding exercise is the extension of previous Naive Bayes classifier program that classifies the email into spam and non spam. Not to worry, if you haven’t gone through Naive Bayes (chapter 1) (Although I would suggest you to complete it first). The same code snippet shall be discussed in abstract way here as well.",
  "translation": "编码练习是以前的Naive Bayes分类程序的扩展，该程序将电子邮件分为垃圾邮件和非垃圾邮件。 不用担心，如果您还没有完成过朴素贝叶斯课程（第1章）（尽管我建议您先完成）。 相同的代码段也将在此处以抽象方式进行讨论。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/freeze/max/60/1*YhLx8eM8_5VS6Bw-9o6QaA.gif?q=20",
  "caption": "Nature of randomness!",
  "type": "image",
  "file": "1!YhLx8eM8_5VS6Bw-9o6QaA.gif"
}, {
  "tag": "H2",
  "text": "1. Download",
  "translation": "1.下载"
}, {
  "tag": "P",
  "text": "I have created a git repository for the data set and the sample code. You can download it from here (Use chapter 3 folder). In case it fails, you can use/refer my version (classifier.py in chapter 3 folder) to understand working.",
  "translation": "我已经为数据集和示例代码创建了一个git存储库。 您可以从此处下载（使用第3章文件夹）。 万一它失败了，您可以使用/参考我的版本（第3章文件夹中的classifier.py）来了解其工作原理。"
}, {
  "tag": "H2",
  "text": "2. Little bit about cleaning",
  "translation": "2.关于清洁的一点"
}, {
  "tag": "P",
  "text": "You may skip this part if you have already gone through coding part of Naive Bayes.(this is for readers who have directly jumped here).",
  "translation": "如果您已经完成Naive Bayes的编码部分，则可以跳过这一部分。（这是针对直接跳到此处的读者的）。"
}, {
  "tag": "P",
  "text": "Before we can apply the sklearn classifiers, we must clean the data. Cleaning involves removal of stop words, extracting most common words from text etc. In the code example concerned we perform following steps:",
  "translation": "在应用sklearn分类器之前，我们必须清除数据。 清理包括删除停用词，从文本中提取最常用的词等。在相关的代码示例中，我们执行以下步骤："
}, {
  "tag": "P",
  "text": "To understand in detail, once again please refer to chapter 1 coding part here.",
  "translation": "要详细了解，请再次参考此处的第1章编码部分。"
}, {
  "tag": "OL",
  "texts": ["Build dictionary of words from email documents from training set.", "Consider the most common 3000 words.", "For each document in training set, create a frequency matrix for these words in dictionary and corresponding labels. [spam email file names start with prefix “spmsg”."],
  "translations": ["从培训集中的电子邮件文档构建单词词典。", "考虑最常见的3000个单词。", "对于训练集中的每个文档，为字典中的这些单词和相应的标签创建一个频率矩阵。 [垃圾邮件的文件名以“ spmsg”开头。"]
}, {
  "tag": "PRE",
  "text": "The code snippet below does this:def make_Dictionary(root_dir):   all_words = []   emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]   for mail in emails:        with open(mail) as m:            for line in m:                words = line.split()                all_words += words   dictionary = Counter(all_words)# if you have python version 3.x use commented version.   # list_to_remove = list(dictionary)   list_to_remove = dictionary.keys()for item in list_to_remove:       # remove if numerical.        if item.isalpha() == False:            del dictionary[item]        elif len(item) == 1:            del dictionary[item]    # consider only most 3000 common words in dictionary.dictionary = dictionary.most_common(3000)return dictionarydef extract_features(mail_dir):  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]  features_matrix = np.zeros((len(files),3000))  train_labels = np.zeros(len(files))  count = 0;  docID = 0;  for fil in files:    with open(fil) as fi:      for i,line in enumerate(fi):        if i == 2:          words = line.split()          for word in words:            wordID = 0            for i,d in enumerate(dictionary):              if d[0] == word:                wordID = i                features_matrix[docID,wordID] = words.count(word)      train_labels[docID] = 0;      filepathTokens = fil.split('/')      lastToken = filepathTokens[len(filepathTokens) - 1]      if lastToken.startswith(\"spmsg\"):          train_labels[docID] = 1;          count = count + 1      docID = docID + 1  return features_matrix, train_labels",
  "translation": "下面的代码段是这样做的：def make_Dictionary（root_dir）：all_words = [] emails = [os.path.join（root_dir，f）for os.listdir（root_dir）中的f]用于电子邮件中的邮件：open（mail） as m：对于m中的行：words = line.split（）all_words =单词字典= Counter（all_words）＃如果您使用的是python 3.x版本，请使用注释版本。 ＃list_to_remove = list（dictionary）list_to_remove = dictionary.keys（）用于list_to_remove中的项目：＃如果为数字则删除。 if item.isalpha（）== False：del dictionary [item] elif len（item）== 1：del dictionary [item]＃仅考虑字典中最多3000个常见单词。dictionary= dictionary.most_common（3000）返回字典def extract_features （mail_dir）：文件= [os.listdir（mail_dir）中fi的os.path.join（mail_dir，fi）] features_matrix = np.zeros（（len（files），3000））train_labels = np.zeros（len（文件））count = 0; docID = 0;对于文件中的fil：使用open（fil）作为fi：对于i，enumerate（fi）中的行：如果i == 2：words = line.split（）对于单词中的单词：wordID = 0对于i，d中的枚举（字典）：如果d [0] ==单词：wordID = i features_matrix [docID，wordID] = words.count（word）train_labels [docID] = 0;如果lastToken.startswith（“ spmsg”）：train_labels [docID] = 1；则filepathTokens = fil.split（'/'）lastToken = filepathTokens [len（filepathTokens）-1]。计数=计数1 docID = docID 1返回features_matrix，train_labels"
}, {
  "tag": "PRE",
  "text": "(本文翻译自Savan Patel的文章《Chapter 3 : Decision Tree Classifier — Coding》，参考：https://medium.com/machine-learning-101/chapter-3-decision-tree-classifier-coding-ae7df4284e99)",
  "translation": "（本文第3章：决策树分类器—编码，参考：https：//medium.com/machine-learning-101/chapter-3-decision-tree-classifier-coding-ae7df4284e99）"
}]