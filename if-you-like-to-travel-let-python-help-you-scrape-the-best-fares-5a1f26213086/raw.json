[{
  "tag": "P",
  "text": "Thank you for reading! If you liked this article, I invite you to check my other stories. I’m mainly interested in Data Science, Python, Blockchain and digital currencies, technology, and a few other things like photography!",
  "translation": "感谢您的阅读！ 如果您喜欢这篇文章，我邀请您检查我的其他故事。 我主要对数据科学，Python，区块链和数字货币，技术以及摄影等其他事物感兴趣！"
}, {
  "tag": "P",
  "text": "If you’d like to get in touch, you can contact me here or simply reply to the article below.",
  "translation": "如果您想取得联系，可以在这里与我联系或直接回复下面的文章。"
}, {
  "tag": "P",
  "text": "Well, every Selenium project starts with a webdriver. I’m using Chromedriver, but there are other alternatives. PhantomJS or Firefox are also popular. After downloading it, place it in a folder and that’s it. These first lines will open a blank Chrome tab.",
  "translation": "好吧，每个Selenium项目都从一个webdriver开始。 我正在使用Chromedriver，但还有其他替代方法。 PhantomJS或Firefox也很流行。 下载后，将其放在文件夹中。 这些第一行将打开一个空白的Chrome标签。"
}, {
  "tag": "P",
  "text": "Please bear in mind I’m not breaking new ground here. There are way more advanced ways of finding cheap deals, but I want my posts to share something simple yet practical!",
  "translation": "请记住，我在这里没有新的突破。 有一些寻找便宜交易的更高级的方法，但是我希望我的帖子分享一些简单而实用的信息！"
}, {
  "tag": "FIGURE",
  "type": "code",
  "raw": "https://gist.github.com/fnneves/907fe74c2b2e99fedbc051e50ed03df2/raw/e68dfb3c74843e8e14b045e9f7ce8cd62e837968/flight_scraper_1.py",
  "code": "from time import sleep, strftime\nfrom random import randint\nimport pandas as pd\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\n\n# Change this to your own chromedriver path!\nchromedriver_path = 'C:/{YOUR PATH HERE}/chromedriver_win32/chromedriver.exe'\n\ndriver = webdriver.Chrome(executable_path=chromedriver_path) # This will open the Chrome window\nsleep(2)"
}, {
  "tag": "P",
  "text": "These are the packages I will use for the whole project. I’ll use randint to make the bot sleep a random number of seconds between each search. That is usually a must have feature for any bot. If you ran the previous code, you should have a Chrome window open, which is where the bot will navigate.",
  "translation": "这些是我将用于整个项目的软件包。 在每次搜索之间，我将使用randint使漫游器随机睡眠数秒。 这通常是所有bot都必须具备的功能。 如果您运行了先前的代码，则应该打开一个Chrome窗口，机器人将在其中导航。"
}, {
  "tag": "P",
  "text": "So let’s make a quick test and go to kayak.com on a different window. Select the cities you want to fly from and to, and the dates. When selecting the dates, make sure you select “+-3 days”. I have written the code with that results page in mind, so there is a high chance you need to make a few adjustments if you want to search specific dates only. I’ll try to point the changes throughout the text, but if you get stuck shoot me a message in the comments.",
  "translation": "因此，让我们进行快速测试，然后在其他窗口上访问kayak.com。 选择您要往返的城市，以及日期。 选择日期时，请确保选择“ + -3天”。 我在编写代码时就考虑到了该结果页面，因此，如果只想搜索特定日期，则很有可能需要进行一些调整。 我会尝试在整个文本中指出所做的更改，但是如果您遇到问题，请在评论中给我留言。"
}, {
  "tag": "P",
  "text": "Hit the search button and get the link in the address bar. It should look something like the link I use below, where I define the variable kayak as the url and execute the get method from the webdriver. Your search results should appear.",
  "translation": "点击搜索按钮，然后在地址栏中获取链接。 它看起来应该类似于我在下面使用的链接，其中我将变量kayak定义为url并从webdriver执行get方法。 您的搜索结果应出现。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*RVnedD9ZGmJaNp8SxFLWsw.png?q=20",
  "type": "image",
  "file": "1!RVnedD9ZGmJaNp8SxFLWsw.png"
}, {
  "tag": "P",
  "text": "Whenever I used the get command more than two or three times in a few minutes, I would be presented with a reCaptcha check. You can actually solve the reCaptcha yourself, and keep doing the tests you want before the next one comes up. From my testing, it seems to be fine for the first search all the times, so it’s really a matter of solving the puzzle yourself if you want to play with the code, and leave the code running by itself with long intervals between them. You really don’t need 10 minute updates on those prices, do you?!",
  "translation": "每当我在几分钟内多次使用get命令两次或三次以上时，都会看到reCaptcha检查。 您实际上可以自己解决reCaptcha，并在下一个测试之前继续进行所需的测试。 从我的测试来看，对于所有时间的第一次搜索似乎都很好，因此，如果您想使用该代码，并让它们彼此间隔很长的时间自行运行，那实际上是一个难题的解决方法。 您真的不需要这些价格的10分钟更新，是吗？"
}, {
  "tag": "H2",
  "text": "Every XPath has its puddle",
  "translation": "每个XPath都有其水坑"
}, {
  "tag": "P",
  "text": "So far we’ve opened a window and got a website. In order to start getting prices and other information, we have to use XPath or CSS selectors. I’ve chosen XPath and didn’t feel the need to mix it up with CSS, but it is perfectly possible to do so. Navigating the webpages with XPath can be confusing, and even if you use the methods I described in the Instagram article, where I use the “copy XPath” trick directly from the inspector view, I realized it’s really not the optimal way to get to the elements you want. Sometimes that link is so specific, that it quickly turns obsolete. The book Web Scraping with Python does a phenomenal job explaining the basics of navigating with XPath and CSS selectors.",
  "translation": "到目前为止，我们已经打开一个窗口并获得了一个网站。 为了开始获取价格和其他信息，我们必须使用XPath或CSS选择器。 我选择了XPath，并没有必要将其与CSS混合使用，但是完全有可能这样做。 使用XPath浏览网页可能会造成混淆，即使您使用我在Instagram文章中描述的方法（我直接在检查器视图中使用“复制XPath”技巧），但我意识到，这实际上并不是到达XPath的最佳方法。 您想要的元素。 有时，该链接是如此具体，以至于很快就会过时。 《 Web Scraping with Python》这本书在解释XPath和CSS选择器导航的基础方面做得非常出色。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*cQMGvsz5DkgxjXVxHqOgGQ.png?q=20",
  "type": "image",
  "file": "1!cQMGvsz5DkgxjXVxHqOgGQ.png"
}, {
  "tag": "P",
  "text": "Moving on, let’s use Python to select the cheapest results. The red text in the code above is the XPath selector, and it can be seen if you right click the webpage anywhere and select “inspect”. Click again with the right click where you want to see the code, and inspect again.",
  "translation": "继续，让我们使用Python选择最便宜的结果。 上面代码中的红色文本是XPath选择器，如果您在任意位置右键单击网页并选择“检查”，则可以看到该文本。 再次单击鼠标右键，然后在您要查看代码的位置单击鼠标，然后再次检查。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*HzaSBG60Wrq-1EE9BnaumQ.png?q=20",
  "type": "image",
  "file": "1!HzaSBG60Wrq-1EE9BnaumQ.png"
}, {
  "tag": "P",
  "text": "To illustrate my previous observation on the shortcomings of copying the path from the inspector, consider these differences:",
  "translation": "为了说明我先前关于从检查器复制路径的缺点的观察，请考虑以下差异："
}, {
  "tag": "PRE",
  "text": "1 # This is what the copy method would return. Right click highlighted rows on the right side and select \"copy > Copy XPath\"//*[@id=\"wtKI-price_aTab\"]/div[1]/div/div/div[1]/div/span/span2 # This is what I used to define the \"Cheapest\" buttoncheap_results = ‘//a[@data-code = “price”]’",
  "translation": "1＃这就是复制方法将返回的内容。 右键单击右侧突出显示的行，然后选择“复制>复制XPath” // * [@ id =“ wtKI-price_aTab”] / div [1] / div / div / div [1] / div / span / span2＃ 这就是我用来定义“最便宜”按钮的内容。cheap_results='// a [@ data-code =“ price”]'"
}, {
  "tag": "P",
  "text": "It’s clearly visible the simplicity of the second option. It searches for an element a which has an attribute data-code equal to price. The first alternative looks for an element with an id equal to wtKI-price_aTab and follows the first div element, four more divs, and two spans. It will work… this time. I can tell you right now that the id element will change next time you load the page. The letters wtKI change dynamically every time a page loads, so your code would be useless as soon as the page reloads. Invest a little time reading about XPath and I promise it will pay off.",
  "translation": "显而易见，第二种选择非常简单。 它搜索元素a，其属性数据代码等于价格。 第一种选择寻找一个id等于wtKI-price_aTab的元素，然后跟随第一个div元素，另外四个div和两个span。 它将工作……这次。 我现在可以告诉您，下次加载页面时，id元素将更改。 字母wtKI每次页面加载时都会动态更改，因此，一旦页面重新加载，您的代码将无用。 花一点时间阅读有关XPath的知识，我保证它会有所回报。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*T59xLGtpqiy-_oAru6tsdw.png?q=20",
  "type": "image",
  "file": "1!T59xLGtpqiy-_oAru6tsdw.png"
}, {
  "tag": "P",
  "text": "Nevertheless, using the copy method will work on less “sophisticated” websites, and that’s fine too!",
  "translation": "不过，使用复制方法在不太“复杂”的网站上也可以使用，也可以！"
}, {
  "tag": "P",
  "text": "Building on what I displayed above, what if we wanted to get all the search results in several strings, inside a list? Simple. Each result is inside an object with the class “resultWrapper”. Fetching all the results can be achieved with a for loop like the next. If you understand this part, you should be able to understand most of the code that will follow. It’s basically pointing to what you want (the results wrapper), using some kind of way (XPath) to get the text there, and placing it in a readable object (first with the flight_containers and then with the flights_list).",
  "translation": "根据我在上面显示的内容，如果我们想以列表中的多个字符串形式获取所有搜索结果，该怎么办？ 简单。 每个结果都在具有“ resultWrapper”类的对象内部。 可以像下面一样使用for循环来获取所有结果。 如果您了解这部分，则应该能够理解后面的大多数代码。 它基本上是指向您想要的内容（结果包装器），使用某种方式（XPath）在此处获取文本，然后将其放置在可读的对象中（首先使用flight_containers，然后使用flight_list）。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*D2pQ7AkjRIsgQCvK1s3SBw.png?q=20",
  "type": "image",
  "file": "1!D2pQ7AkjRIsgQCvK1s3SBw.png"
}, {
  "tag": "P",
  "text": "The first 3 rows are displayed and we can clearly see everything we need, but we have better alternatives to get the information. We need to scrape each element individually.",
  "translation": "将显示前三行，我们可以清楚地看到我们需要的所有内容，但是我们有更好的替代方法来获取信息。 我们需要分别抓取每个元素。"
}, {
  "tag": "H2",
  "text": "Clear for take-off!",
  "translation": "清除起飞！"
}, {
  "tag": "P",
  "text": "The easiest function to code is to load more results, so let’s start with that. I want to maximize the amount of flights I get, without triggering the security check, so I will click once in the “load more results” button every time a page is displayed. The only thing new is the try statement, which I added because sometimes the button was not loading properly. If it acts up with you too, simply comment it out in the start_kayak function that I will show ahead.",
  "translation": "编写代码最简单的功能是加载更多结果，因此我们从此开始。 我想最大化我获得的航班数量，而又不会触发安全检查，因此每次显示页面时，我都会在“加载更多结果”按钮中单击一次。 唯一的新功能是try语句，我添加了它，因为有时按钮无法正确加载。 如果它也对您有帮助，只需在我将要显示的start_kayak函数中将其注释掉即可。"
}, {
  "tag": "FIGURE",
  "type": "code",
  "raw": "https://gist.github.com/fnneves/5fe39f883fab848fef022b76e87169d3/raw/137581e998d215f61cd16da39320f2ee2c16c718/flight_scraper_2.py",
  "code": "# Load more results to maximize the scraping\ndef load_more():\n    try:\n        more_results = '//a[@class = \"moreButton\"]'\n        driver.find_element_by_xpath(more_results).click()\n        # Printing these notes during the program helps me quickly check what it is doing\n        print('sleeping.....')\n        sleep(randint(45,60))\n    except:\n        pass"
}, {
  "tag": "P",
  "text": "And now, after a long intro (I can get carried away at times!) we’re ready to define the function that will actually scrape the pages.",
  "translation": "现在，经过漫长的介绍（有时我可能会迷住了！），我们已经准备好定义将实际抓取页面的功能。"
}, {
  "tag": "P",
  "text": "I already compiled most of the elements in the next function called page_scrape. Sometimes, the elements returned lists interpolating first and second legs information. I used a simple method to split them, for instance in the first section_a_list and section_b_list variables. The function also returns a dataframe flights_df, so we can separate the results we get in the different sorts and merge them later.",
  "translation": "我已经在称为page_scrape的下一个函数中编译了大多数元素。 有时，返回的元素会列出内插第一和第二条腿信息的列表。 我使用了一种简单的方法来拆分它们，例如在第一个section_a_list和section_b_list变量中。 该函数还返回一个数据帧flight_df，因此我们可以将得到的结果分开，并在以后进行合并。"
}, {
  "tag": "FIGURE",
  "type": "code",
  "raw": "https://gist.github.com/fnneves/0e3217d31f392e40ac0da990bc92e37a/raw/8fc02cbc41794b59c14b531c831454fe09813f47/flight_scraper_4.py",
  "code": "def page_scrape():\n    \"\"\"This function takes care of the scraping part\"\"\"\n    \n    xp_sections = '//*[@class=\"section duration\"]'\n    sections = driver.find_elements_by_xpath(xp_sections)\n    sections_list = [value.text for value in sections]\n    section_a_list = sections_list[::2] # This is to separate the two flights\n    section_b_list = sections_list[1::2] # This is to separate the two flights\n    \n    # if you run into a reCaptcha, you might want to do something about it\n    # you will know there's a problem if the lists above are empty\n    # this if statement lets you exit the bot or do something else\n    # you can add a sleep here, to let you solve the captcha and continue scraping\n    # i'm using a SystemExit because i want to test everything from the start\n    if section_a_list == []:\n        raise SystemExit\n    \n    # I'll use the letter A for the outbound flight and B for the inbound\n    a_duration = []\n    a_section_names = []\n    for n in section_a_list:\n        # Separate the time from the cities\n        a_section_names.append(''.join(n.split()[2:5]))\n        a_duration.append(''.join(n.split()[0:2]))\n    b_duration = []\n    b_section_names = []\n    for n in section_b_list:\n        # Separate the time from the cities\n        b_section_names.append(''.join(n.split()[2:5]))\n        b_duration.append(''.join(n.split()[0:2]))\n\n    xp_dates = '//div[@class=\"section date\"]'\n    dates = driver.find_elements_by_xpath(xp_dates)\n    dates_list = [value.text for value in dates]\n    a_date_list = dates_list[::2]\n    b_date_list = dates_list[1::2]\n    # Separating the weekday from the day\n    a_day = [value.split()[0] for value in a_date_list]\n    a_weekday = [value.split()[1] for value in a_date_list]\n    b_day = [value.split()[0] for value in b_date_list]\n    b_weekday = [value.split()[1] for value in b_date_list]\n    \n    # getting the prices\n    xp_prices = '//a[@class=\"booking-link\"]/span[@class=\"price option-text\"]'\n    prices = driver.find_elements_by_xpath(xp_prices)\n    prices_list = [price.text.replace('$','') for price in prices if price.text != '']\n    prices_list = list(map(int, prices_list))\n\n    # the stops are a big list with one leg on the even index and second leg on odd index\n    xp_stops = '//div[@class=\"section stops\"]/div[1]'\n    stops = driver.find_elements_by_xpath(xp_stops)\n    stops_list = [stop.text[0].replace('n','0') for stop in stops]\n    a_stop_list = stops_list[::2]\n    b_stop_list = stops_list[1::2]\n\n    xp_stops_cities = '//div[@class=\"section stops\"]/div[2]'\n    stops_cities = driver.find_elements_by_xpath(xp_stops_cities)\n    stops_cities_list = [stop.text for stop in stops_cities]\n    a_stop_name_list = stops_cities_list[::2]\n    b_stop_name_list = stops_cities_list[1::2]\n    \n    # this part gets me the airline company and the departure and arrival times, for both legs\n    xp_schedule = '//div[@class=\"section times\"]'\n    schedules = driver.find_elements_by_xpath(xp_schedule)\n    hours_list = []\n    carrier_list = []\n    for schedule in schedules:\n        hours_list.append(schedule.text.split('\\n')[0])\n        carrier_list.append(schedule.text.split('\\n')[1])\n    # split the hours and carriers, between a and b legs\n    a_hours = hours_list[::2]\n    a_carrier = carrier_list[::2]\n    b_hours = hours_list[1::2]\n    b_carrier = carrier_list[1::2]\n\n    \n    cols = (['Out Day', 'Out Time', 'Out Weekday', 'Out Airline', 'Out Cities', 'Out Duration', 'Out Stops', 'Out Stop Cities',\n            'Return Day', 'Return Time', 'Return Weekday', 'Return Airline', 'Return Cities', 'Return Duration', 'Return Stops', 'Return Stop Cities',\n            'Price'])\n\n    flights_df = pd.DataFrame({'Out Day': a_day,\n                               'Out Weekday': a_weekday,\n                               'Out Duration': a_duration,\n                               'Out Cities': a_section_names,\n                               'Return Day': b_day,\n                               'Return Weekday': b_weekday,\n                               'Return Duration': b_duration,\n                               'Return Cities': b_section_names,\n                               'Out Stops': a_stop_list,\n                               'Out Stop Cities': a_stop_name_list,\n                               'Return Stops': b_stop_list,\n                               'Return Stop Cities': b_stop_name_list,\n                               'Out Time': a_hours,\n                               'Out Airline': a_carrier,\n                               'Return Time': b_hours,\n                               'Return Airline': b_carrier,                           \n                               'Price': prices_list})[cols]\n    \n    flights_df['timestamp'] = strftime(\"%Y%m%d-%H%M\") # so we can know when it was scraped\n    return flights_df"
}, {
  "tag": "P",
  "text": "I’ve tried to make the names clear to follow. Remember that the variables with a are related with the first leg of the trip, and b with the second. On to the next function.",
  "translation": "我试图使名称清楚易懂。 请记住，带有a的变量与行程的第一段相关，而与b的变量与行程的第一段相关。 转到下一个功能。"
}, {
  "tag": "H2",
  "text": "Wait, there’s more?!",
  "translation": "等等，还有吗？"
}, {
  "tag": "P",
  "text": "So far we have a function to load more results, and a function to scrape those results. I could end the article here and you would still be able to use these manually and use the scraping function on a page you browsed by yourself, but I did mention something about sending an email to yourself and some other information! That is all inside the next function start_kayak!",
  "translation": "到目前为止，我们具有加载更多结果的功能，以及刮取这些结果的功能。 我可以在此处结束本文，您仍然可以手动使用它们，并在自己浏览的页面上使用抓取功能，但是我确实提到了有关向自己发送电子邮件的信息以及其他一些信息！ 这就是下一个函数start_kayak的全部内容！"
}, {
  "tag": "FIGURE",
  "type": "code",
  "raw": "https://gist.github.com/fnneves/416d8f08d14f1b1d132c01f9e8f01020/raw/fc3fad62db6fc1fb90097d05d1db3c53b2c5e2b9/flight_scraper_3.py",
  "code": "def start_kayak(city_from, city_to, date_start, date_end):\n    \"\"\"City codes - it's the IATA codes!\n    Date format -  YYYY-MM-DD\"\"\"\n    \n    kayak = ('https://www.kayak.com/flights/' + city_from + '-' + city_to +\n             '/' + date_start + '-flexible/' + date_end + '-flexible?sort=bestflight_a')\n    driver.get(kayak)\n    sleep(randint(8,10))\n    \n    # sometimes a popup shows up, so we can use a try statement to check it and close\n    try:\n        xp_popup_close = '//button[contains(@id,\"dialog-close\") and contains(@class,\"Button-No-Standard-Style close \")]'\n        driver.find_elements_by_xpath(xp_popup_close)[5].click()\n    except Exception as e:\n        pass\n    sleep(randint(60,95))\n    print('loading more.....')\n    \n#     load_more()\n    \n    print('starting first scrape.....')\n    df_flights_best = page_scrape()\n    df_flights_best['sort'] = 'best'\n    sleep(randint(60,80))\n    \n    # Let's also get the lowest prices from the matrix on top\n    matrix = driver.find_elements_by_xpath('//*[contains(@id,\"FlexMatrixCell\")]')\n    matrix_prices = [price.text.replace('$','') for price in matrix]\n    matrix_prices = list(map(int, matrix_prices))\n    matrix_min = min(matrix_prices)\n    matrix_avg = sum(matrix_prices)/len(matrix_prices)\n    \n    print('switching to cheapest results.....')\n    cheap_results = '//a[@data-code = \"price\"]'\n    driver.find_element_by_xpath(cheap_results).click()\n    sleep(randint(60,90))\n    print('loading more.....')\n    \n#     load_more()\n    \n    print('starting second scrape.....')\n    df_flights_cheap = page_scrape()\n    df_flights_cheap['sort'] = 'cheap'\n    sleep(randint(60,80))\n    \n    print('switching to quickest results.....')\n    quick_results = '//a[@data-code = \"duration\"]'\n    driver.find_element_by_xpath(quick_results).click()  \n    sleep(randint(60,90))\n    print('loading more.....')\n    \n#     load_more()\n    \n    print('starting third scrape.....')\n    df_flights_fast = page_scrape()\n    df_flights_fast['sort'] = 'fast'\n    sleep(randint(60,80))\n    \n    # saving a new dataframe as an excel file. the name is custom made to your cities and dates\n    final_df = df_flights_cheap.append(df_flights_best).append(df_flights_fast)\n    final_df.to_excel('search_backups//{}_flights_{}-{}_from_{}_to_{}.xlsx'.format(strftime(\"%Y%m%d-%H%M\"),\n                                                                                   city_from, city_to, \n                                                                                   date_start, date_end), index=False)\n    print('saved df.....')\n    \n    # We can keep track of what they predict and how it actually turns out!\n    xp_loading = '//div[contains(@id,\"advice\")]'\n    loading = driver.find_element_by_xpath(xp_loading).text\n    xp_prediction = '//span[@class=\"info-text\"]'\n    prediction = driver.find_element_by_xpath(xp_prediction).text\n    print(loading+'\\n'+prediction)\n    \n    # sometimes we get this string in the loading variable, which will conflict with the email we send later\n    # just change it to \"Not Sure\" if it happens\n    weird = '¯\\\\_(ツ)_/¯'\n    if loading == weird:\n        loading = 'Not sure'\n    \n    username = 'YOUREMAIL@hotmail.com'\n    password = 'YOUR PASSWORD'\n\n    server = smtplib.SMTP('smtp.outlook.com', 587)\n    server.ehlo()\n    server.starttls()\n    server.login(username, password)\n    msg = ('Subject: Flight Scraper\\n\\n\\\nCheapest Flight: {}\\nAverage Price: {}\\n\\nRecommendation: {}\\n\\nEnd of message'.format(matrix_min, matrix_avg, (loading+'\\n'+prediction)))\n    message = MIMEMultipart()\n    message['From'] = 'YOUREMAIL@hotmail.com'\n    message['to'] = 'YOUROTHEREMAIL@domain.com'\n    server.sendmail('YOUREMAIL@hotmail.com', 'YOUROTHEREMAIL@domain.com', msg)\n    print('sent email.....')"
}, {
  "tag": "P",
  "text": "It requires you to declare the cities and the dates. From there, it will open the address in the kayak string, which goes directly to the sort by “best” results page. After the first scrape, I took the liberty of getting the top matrix with the prices. It will be used to calculate an average and a minimum, to be sent in the email along with Kayak’s prediction (in the page it should be on the top left). This is one of the things that could cause an error on a single date search since there is no matrix element there.",
  "translation": "它要求您声明城市和日期。 从那里，它将打开皮划艇字符串中的地址，该地址将直接转到“最佳”结果页面的排序。 第一次刮擦之后，我自由地获得了价格最高的矩阵。 它将用于计算平均值和最小值，并与Kayak的预测一起发送到电子邮件中（在页面中，该值应该在左上方）。 这是可能导致单日搜索错误的原因之一，因为那里没有矩阵元素。"
}, {
  "tag": "P",
  "text": "I tested this using an Outlook account (hotmail.com). Although I did not test it using a Gmail account to send the email, there are lots of alternatives you can search, and the book I mentioned earlier has other ways to do it too. If you already have a Hotmail account, it should work if you replace your details.",
  "translation": "我使用Outlook帐户（hotmail.com）对此进行了测试。 尽管我没有使用Gmail帐户发送电子邮件来测试它，但是您可以搜索很多替代方法，而我前面提到的书也有其他方法可以做到这一点。 如果您已经有Hotmail帐户，则替换您的详细信息后，该帐户应该可以使用。"
}, {
  "tag": "P",
  "text": "If you want to explore what some parts of the script are doing, please copy it and use it outside the functions. That is the only way you will fully understand it",
  "translation": "如果您想探索脚本的某些部分在做什么，请复制它并在功能之外使用它。 这是您完全了解它的唯一方法"
}, {
  "tag": "H2",
  "text": "Using everything we just created",
  "translation": "使用我们刚刚创建的所有内容"
}, {
  "tag": "P",
  "text": "After all this, we might as well come up with a simple loop to start using the functions we just created and keep them busy. Completed with four “fancy” prompts for you to actually write the cities and dates (the inputs). Since when we’re testing, we don’t want to type these variables every time, alternate it with the explicit way below them when needed.",
  "translation": "完成所有这些之后，我们不妨提出一个简单的循环来开始使用我们刚刚创建的功能并使它们保持忙碌状态。 带有四个“精美”提示，提示您实际输入城市和日期（输入）。 由于进行测试时，我们不想每次都键入这些变量，因此在需要时用它们下面的显式方式替换它们。"
}, {
  "tag": "FIGURE",
  "type": "code",
  "raw": "https://gist.github.com/fnneves/a2c958e1ef329509d9291b8dec56fac6/raw/a3f1d92e399700c583e04cfcf65ad09882e15715/flight_scraper_5.py",
  "code": "city_from = input('From which city? ')\ncity_to = input('Where to? ')\ndate_start = input('Search around which departure date? Please use YYYY-MM-DD format only ')\ndate_end = input('Return when? Please use YYYY-MM-DD format only ')\n\n# city_from = 'LIS'\n# city_to = 'SIN'\n# date_start = '2019-08-21'\n# date_end = '2019-09-07'\n\nfor n in range(0,5):\n    start_kayak(city_from, city_to, date_start, date_end)\n    print('iteration {} was complete @ {}'.format(n, strftime(\"%Y%m%d-%H%M\")))\n    \n    # Wait 4 hours\n    sleep(60*60*4)\n    print('sleep finished.....')"
}, {
  "tag": "P",
  "text": "If you made it this far, congratulations! There are plenty of improvements I can think of, like integrating with Twilio to send you a text message instead of an email. You can also use VPN’s or more obscure ways to grind the search results from several servers at the same time. There’s the captcha issue, that may pop up from time to time, but there are workarounds for these sorts of things. I think you have some pretty solid basis here, and I encourage you to try and add some extra features. Maybe you want the excel file sent as an attachment. I always welcome constructive feedback, so feel free to leave a comment below. I try to respond to every one!",
  "translation": "如果您做到了这一点，那么恭喜！ 我可以想到很多改进，例如与Twilio集成以向您发送文本消息而不是电子邮件。 您还可以使用VPN或更模糊的方式来同时搜索来自多个服务器的搜索结果。 验证码问题可能会不时弹出，但是对于此类问题有一些解决方法。 我认为您在这里有一定的扎实基础，建议您尝试添加一些额外的功能。 也许您希望将Excel文件作为附件发送。 我一直欢迎建设性的反馈，请随时在下面发表评论。 我试图回应每一个人！"
}, {
  "tag": "P",
  "text": "By popular request in the comments section, here’s the link to a full Jupyter Notebook with all the code!",
  "translation": "根据评论部分的普遍要求，以下是完整的Jupyter Notebook以及所有代码的链接！"
}, {
  "tag": "P",
  "text": "And here’s the link for a video where I go over the code. I’m curious to see if you guys actually watch it, and any feedback is welcome. Is it too long? Should I create shorter videos and go over the code in more detail? Let me know :)",
  "translation": "这是我查看代码的视频的链接。 我很想知道你们是否真的在看它，欢迎大家提供反馈。 太久了吗 我是否应该制作较短的视频并更详细地检查代码？ 让我知道 ：）"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/27/1*MzayW8dDbTl1bOGCRVTxFg.png?q=20",
  "caption": "example of a test run with the script",
  "type": "image",
  "file": "1!MzayW8dDbTl1bOGCRVTxFg.png"
}, {
  "tag": "P",
  "text": "If you want to learn more about Web Scraping, I strongly recommend the book Web Scraping with Python. I really liked the examples and the clear explanation of how the code is working. And if you prefer social media scraping, there’s also an excellent book exclusively on the subject. I’m using the latter for my next article using the Twitter API, but there is stuff there to scrape even LinkedIn! (If you decide to purchase and use my links, I receive a small fee at no extra cost to you. I do need a lot of coffee to write these articles! Thanks in advance!)",
  "translation": "如果您想了解有关Web Scraping的更多信息，我强烈建议您阅读《用Python进行Web Scraping》一书。 我真的很喜欢这些示例，并对代码的工作方式进行了清晰的解释。 而且，如果您更喜欢社交媒体抓取功能，那么还有一本专门针对该主题的绝妙书籍。 我将在下一篇使用Twitter API的文章中使用后者，但是这里还有很多东西甚至可以吸引LinkedIn。 （如果您决定购买和使用我的链接，我会收到一笔小小的费用，而您无需支付任何额外费用。我确实需要大量的咖啡才能撰写这些文章！在此先感谢您！）"
}, {
  "tag": "H2",
  "text": "Simply put",
  "translation": "简单的说"
}, {
  "tag": "P",
  "text": "The goal of this project is to build a web scraper that will run and perform searches on flight prices with flexible dates (up to 3 days before and after the dates you select first), for a particular destination. It saves an excel with the results and sends an email with the quick stats. Obviously, the objective is to help us find the best deals!",
  "translation": "该项目的目标是构建一个网络刮板，该刮板将针对特定目的地运行并以灵活的日期（最先选择的日期之前和之后的3天）对航班价格进行搜索。 它将结果保存为excel，并发送包含快速统计信息的电子邮件。 显然，目标是帮助我们找到最优惠的价格！"
}, {
  "tag": "P",
  "text": "If you get lost in some part, try to have a look at my article about the Instagram bot, as it uses Selenium too.",
  "translation": "如果您迷失了方向，请尝试看看我有关Instagram机器人的文章，因为它也使用Selenium。"
}, {
  "tag": "P",
  "text": "The real life application for this is up to you. I’ve used it to search both holidays and recently also some short trips to my hometown!",
  "translation": "实际应用取决于您。 我用它来搜索假期和最近去我家乡的短途旅行！"
}, {
  "tag": "P",
  "text": "If you’re serious about it, you can run the script on a server (a simple Raspberry Pi will do), and make it start once or twice each day. The results will be mailed to you, and I suggest saving the excel file to a Dropbox folder, so you can access it from anywhere, anytime.",
  "translation": "如果您对此很认真，则可以在服务器上运行脚本（一个简单的Raspberry Pi可以运行），并使其每天启动一次或两次。 结果将被邮寄给您，我建议将excel文件保存到Dropbox文件夹中，以便您可以随时随地访问它。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*7lPZGAGwf5wK0s-S09hd2g.png?q=20",
  "caption": "I did not find any error fare yet, but I suppose it’s possible!",
  "type": "image",
  "file": "1!7lPZGAGwf5wK0s-S09hd2g.png"
}, {
  "tag": "P",
  "text": "It searches through “flexible dates” so it will look for flights up to 3 days before and after the dates you select first. Although the script works for one pair of destinations at a time, you can easily adapt it to run several inside each loop. You might even end up finding some error fares… which would be awesome!",
  "translation": "它搜索“弹性日期”，因此它将查找您最先选择的日期前后3天以内的航班。 尽管该脚本一次可用于一对目的地，但是您可以轻松地对其进行调整，以在每个循环中运行多个目的地。 您甚至可能会发现一些错误的票价……这太棒了！"
}, {
  "tag": "H2",
  "text": "Yet another scraper",
  "translation": "另一个刮板"
}, {
  "tag": "P",
  "text": "When I first started to do some web scraping I was not particularly interested in the topic. There… I said it! I wanted to do more projects with predictive modeling, financial analysis and maybe some sentiment analysis, but it turns out that it was a lot of fun figuring out how to build the first web crawler. As I keep learning, I realized web scraping is what makes the internet “work”.",
  "translation": "当我第一次开始进行网络抓取时，我对该主题并不特别感兴趣。 那里...我说了！ 我想通过预测建模，财务分析以及一些情感分析来做更多的项目，但是事实证明弄清楚如何构建第一个Web爬网程序确实很有趣。 在不断学习的过程中，我意识到使Web变得“正常工作”的原因是网络抓取。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*KOq55_B5j4bFz556HdrWTA.jpeg?q=20",
  "caption": "Yep… Just like Larry and Sergey, you can hit the jacuzzi after you initiate the scraper! (image: wired.com)",
  "type": "image",
  "file": "1!KOq55_B5j4bFz556HdrWTA.jpeg"
}, {
  "tag": "P",
  "text": "You might think it’s a really bold claim, but what if I told you that Google started out with a web scraper Larry Page built with Java and Python? It crawled, and still does, the whole internet trying to provide you the best possible answer for your questions. There are endless applications for web scraping, and even if you prefer other subjects in Data Science, you’ll still need some scraping skills to get your data.",
  "translation": "您可能会认为这是一个非常大胆的主张，但是如果我告诉您Google最初是使用Java和Python构建的网页抓取工具Larry Page，该怎么办？ 它遍历了整个互联网，并且至今仍在努力为您的问题提供最佳答案。 Web抓取应用层出不穷，即使您更喜欢Data Science中的其他主题，您仍然需要一些抓取技巧来获取数据。"
}, {
  "tag": "P",
  "text": "Some of the techniques I use here come from an awesome book I recently bought that covers everything related with web scraping. Plenty of simple examples and lots of practical applications. There’s even a very interesting chapter about solving reCaptcha checks which blew my mind — I was not aware of the existing tools and even services to deal with it! (Disclaimer: if you purchase the book through my link, I receive a small fee at no extra cost to you. So if you feel like buying me a coffee by the end of this article, I appreciate it!)",
  "translation": "我在这里使用的一些技术来自我最近买的一本很棒的书，其中涵盖了与Web抓取有关的所有内容。 很多简单的例子和大量的实际应用。 甚至有一章非常有趣的章节涉及解决reCaptcha检查，这让我大吃一惊-我不知道现有的工具甚至服务都无法解决！ （免责声明：如果您通过我的链接购买了这本书，那么我将获得一笔小额费用，而无需您支付任何额外费用。因此，如果您希望在本文结束时为我购买咖啡，我将不胜感激！）"
}, {
  "tag": "H2",
  "text": "“Do you like traveling?!”",
  "translation": "“你喜欢旅行吗？！”"
}, {
  "tag": "P",
  "text": "This simple and innocuous question often meets a positive answer and a subsequent story or two about a previous adventure. Most of us would agree that traveling is a great way to experience new cultures and broaden our own perspectives. But if the question was “Do you like the process of searching for plane tickets?”, I’m sure the reaction would be a lot less enthusiastic…",
  "translation": "这个简单而无害的问题通常会得到一个肯定的答案以及一个或多个关于先前冒险的故事。 我们大多数人都会同意，旅行是体验新文化并拓宽视野的绝佳途径。 但是，如果问题是“您喜欢搜索机票的过程吗？”，那么我敢肯定反应会不那么热情……"
}, {
  "tag": "P",
  "text": "Python to the rescue.",
  "translation": "Python解救。"
}, {
  "tag": "P",
  "text": "The first challenge was to choose which platform to scrape the information from. It was not easy, but I settled with Kayak. I tried Momondo, Skyscanner, Expedia and a few more, but the reCaptchas on those websites were ruthless. After a few attempts selecting traffic lights, crosswalks and bicycles in those “are you human” checks, I concluded Kayak was my best alternative even though it throws out a security check if you load too many pages in a short period of time. I managed to keep the bot querying the website in 4 to 6 hour intervals and it was ok. There may be an occasional hiccup here and there, but if you start getting reCaptcha checks, either solve them manually and start the bot after that, or wait a few hours and it should reset. Feel free to adapt the code to another platform, and you’re welcome to share it in the comments section!",
  "translation": "第一个挑战是选择从哪个平台抓取信息。 这并不容易，但是我和皮划艇定居了。 我尝试了Momondo，Skyscanner，Expedia等等，但是这些网站上的reCaptchas都很残酷。 经过几次尝试在“您是人类”检查中选择交通信号灯，人行横道和自行车后，我得出结论认为，即使您在短时间内加载过多页面也会导致安全检查，但Kayak是我的最佳选择。 我设法使该漫游器每隔4到6个小时就查询一次网站，而且还可以。 有时可能会偶尔出现打h，但是如果您开始获得reCaptcha检查，请手动解决它们，然后启动bot，或者等待几个小时，然后将其重置。 随时将代码改编成其他平台，欢迎在评论部分中共享它！"
}, {
  "tag": "P",
  "text": "If you’re new to web scraping or if you don’t know why some websites go a long way to prevent it, please do yourself a big favor before writing your first line of code towards a scraper. Google “web scraping etiquette”. Your endeavour might be over much sooner than you think if you just start scraping like a madman.",
  "translation": "如果您不熟悉网页抓取功能，或者不知道为什么有些网站在阻止它方面大有帮助，请在编写第一行代码之前将您的工作大功告成。 Google“网络抓取礼节”。 如果您像疯子一样开始爬行，您的努力可能会比您想像的要早得多。"
}, {
  "tag": "H2",
  "text": "Fasten your seatbelts…",
  "translation": "系好安全带…"
}, {
  "tag": "P",
  "text": "Pun intended",
  "translation": "双关语意"
}, {
  "tag": "P",
  "text": "After importing and opening a chrome tab, we’ll define some functions that will be used inside a loop. The idea of the structure is more or less like this:",
  "translation": "导入并打开Chrome浏览器标签后，我们将定义一些将在循环内使用的函数。 结构的想法或多或少是这样的："
}, {
  "tag": "UL",
  "texts": ["a function will start the bot, declaring the cities and dates we want to search", "this function gets the first search results, sorted by “best” flights, and clicks the “load more results”", "another function will scrape the whole page, and return a dataframe", "it will repeat step 2 and 3 for the “cheap” and “fastest” sort types", "an email will be sent to you with a brief summary of the prices (cheapest and average), and the dataframe with the three sort types will be saved as an excel file", "all the previous steps are repeated in a loop, that runs every X hours."],
  "translations": ["一个功能将启动机器人，并声明我们要搜索的城市和日期", "此功能获取第一个搜索结果，并按“最佳”飞行排序，然后单击“加载更多结果”", "另一个函数将抓取整个页面，并返回一个数据框", "它将针对“便宜”和“最快”排序类型重复步骤2和3", "将会向您发送一封电子邮件，其中包含价格的简要摘要（最低价和均价），并将具有三种排序类型的数据框保存为excel文件", "每隔X个小时运行一次，便会重复执行前面的所有步骤。"]
}, {
  "tag": "H1",
  "text": "If you like to travel, let Python help you scrape the best cheap flights!",
  "translation": "如果您喜欢旅行，请让Python帮助您抓取最好的廉价航班！"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*v9AS5Lct9YUAx8qlAJghhg.png?q=20",
  "caption": "source: videoblocks.com",
  "type": "image",
  "file": "1!v9AS5Lct9YUAx8qlAJghhg.png"
}, {
  "tag": "P",
  "text": "It’s been a while since I published my last article, showing how to build a web scraper to increase your Instagram audience. So much has happened in the meantime… A new job, a move to a new country, a participation as keynote speaker in a PyData event in Lisbon, a lot of unread messages sitting in my inbox(es), and a growing follower count on Medium that completely took me by surprise — almost 1k followers! I honestly did not expect such an amazing feedback. I really wanted to make a note here and not only thank everyone who read my previous posts, but also to assure those who actually clicked “Follow” that I am still here and new articles will keep coming hopefully on a more regular basis. If you don’t follow me yet… did I mention that I’m really really close to the incredibly motivating milestone of 1000 followers? :)",
  "translation": "自从我发表我的上一篇文章已经有一段时间了，展示了如何构建网络抓取工具来增加您的Instagram受众。 与此同时，发生了很多事情……新工作，搬到新国家，以主讲人的身份参加了在里斯本举行的PyData活动，收件箱中有许多未读邮件，并且越来越多的关注者 完全让我感到惊讶的媒介-将近1000个关注者！ 老实说，我没想到会有如此惊人的反馈。 我真的想在这里做笔记，不仅要感谢阅读我以前的帖子的每个人，还要向那些真正单击“关注”的人保证我仍然在这里，新文章有望继续定期发行。 如果您还不关注我，我是否提到我真的很接近1000个关注者的令人激动的里程碑？ :)"
}, {
  "tag": "P",
  "text": "update: I’ve created a video where I go over the code. Should be easier to follow!",
  "translation": "更新：我创建了一个视频，在其中我检查了代码。 应该更容易遵循！"
}, {
  "tag": "PRE",
  "text": "(本文翻译自Fábio Neves的文章《If you like to travel, let Python help you scrape the best cheap flights!》，参考：https://towardsdatascience.com/if-you-like-to-travel-let-python-help-you-scrape-the-best-fares-5a1f26213086)",
  "translation": "（本文翻译自FábioNeves的文章《如果您喜欢旅行，请让Python帮助您进行最好的廉价航班！》，参考：https：//towardsdatascience.com/if-you-like-to-travel-let-python -帮助您刮擦最好的票价5a1f26213086）"
}]