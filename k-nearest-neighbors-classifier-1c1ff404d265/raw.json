[{
  "tag": "H1",
  "text": "Final Thoughts",
  "translation": "最后的想法"
}, {
  "tag": "P",
  "text": "Except brute force, K Nearest Neighbors implements tree like data structure to determine the distances from point of interest to points in training set. The selection of best algorithm depends on sparsity of data, number of neighbors requested and the dimension/number of features.",
  "translation": "除蛮力外，K最近邻实现树状数据结构来确定从兴趣点到训练集中点的距离。 最佳算法的选择取决于数据的稀疏性，请求的邻居数以及要素的维数/数量。"
}, {
  "tag": "P",
  "text": "I hope that this section was helpful in understanding the working behind K Nearest Neighbor classifier. Comment down your thoughts, feedback or suggestions if any below. If you liked this post, share it with your friends, subscribe to Machine Learning 101 click the heart(❤) icon. Follow me on Facebook, twitter, LinkedIn. Peace!",
  "translation": "我希望本节对理解K最近邻分类器的工作有所帮助。 如果您有任何意见，建议或建议，请在下方写下来。 如果您喜欢此帖子，请与您的朋友分享，并订阅Machine Learning 101，单击heart（❤）图标。 在Facebook，Twitter，LinkedIn上关注我。 和平！"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/36/1*myAZ--cmGzP9t1vsKvOjaA.jpeg?q=20",
  "caption": "I would say, Nature of machine learning algorithms :P",
  "type": "image",
  "file": "1!myAZ--cmGzP9t1vsKvOjaA.jpeg"
}, {
  "tag": "H2",
  "text": "Some important Parameters",
  "translation": "一些重要参数"
}, {
  "tag": "P",
  "text": "n_neighbors : Number of neighbors to consider. Default is 5.",
  "translation": "n_neighbors：要考虑的邻居数。 默认值为5。"
}, {
  "tag": "P",
  "text": "algorithm: By default it is set to auto. Algorithm can be {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}.",
  "translation": "算法：默认情况下设置为自动。 算法可以是{'auto'，'ball_tree'，'kd_tree'，'brute'}。"
}, {
  "tag": "P",
  "text": "Currently, algorithm = ‘auto’ selects ‘kd_tree’ if k < N/2 and the ‘effective_metric_’ is in the ‘VALID_METRICS’ list of ‘kd_tree’. It selects ‘ball_tree’ if k < N/2 and the ‘effective_metric_’ is not in the ‘VALID_METRICS’ list of ‘kd_tree’. It selects ‘brute’ if k >= N/2. This choice is based on the assumption that the number of query points is at least the same order as the number of training points, and that leaf_size is close to its default value of 30. [ reference : Sklearn Documentation].",
  "translation": "当前，如果k <N / 2且'effective_metric_'在'kd_tree'的'VALID_METRICS'列表中，算法='自动'选择'kd_tree'。 如果k <N / 2且'effective_metric_'不在'kd_tree'的'VALID_METRICS'列表中，则选择'ball_tree'。 如果k> = N / 2，则选择“粗略”。 该选择基于以下假设：查询点的数量至少与训练点的数量相同，并且leaf_size接近其默认值30。[参考：Sklearn文档]。"
}, {
  "tag": "P",
  "text": "You can explore all the parameters here.",
  "translation": "您可以在此处浏览所有参数。"
}, {
  "tag": "H1",
  "text": "Sklearn K nearest and parameters",
  "translation": "Sklearn K最近和参数"
}, {
  "tag": "P",
  "text": "Sklearn in python provides implementation for K Nearest Neighbors Classifier. Below is sample code snippet to use in python:",
  "translation": "python中的Sklearn提供了K最近邻分类器的实现。 以下是在python中使用的示例代码片段："
}, {
  "tag": "PRE",
  "text": "from sklearn.neighbors import KNeighborsClassifierneigh = KNeighborsClassifier(n_neighbors=3)neigh.fit(features_matrix, labels)predicted_values = neigh.predict(test_matrix)",
  "translation": "从sklearn.neighbors导入KNeighborsClassifierneigh = KNeighborsClassifier（n_neighbors = 3）neigh.fit（features_matrix，标签）predicted_values = neigh.predict（test_matrix）"
}, {
  "tag": "H2",
  "text": "Ball Tree",
  "translation": "球树"
}, {
  "tag": "P",
  "text": "Ball tree assumes the data in multidimensional dimensional space and creates the nested hyper spheres. Query Time complexity is O[Dlog(N)].",
  "translation": "球树在多维空间中假设数据，并创建嵌套的超球体。 查询时间复杂度为O [Dlog（N）]。"
}, {
  "tag": "H1",
  "text": "Which one to choose when?",
  "translation": "何时选择哪一个？"
}, {
  "tag": "P",
  "text": "The selection of the relevant algorithm for problem at hand depends on the number of dimensions and the size of training set.",
  "translation": "针对眼前问题的相关算法的选择取决于维数和训练集的大小。"
}, {
  "tag": "OL",
  "texts": ["For small sample size and small dimensions, brute force performs well.", "Sparsity of data : If data is sparse with small dimensions (< 20) KD tree will perform better than Ball Tree algorithm.", "Value of K (neighbors) : As the K increases, query time of both KD tree and Ball tree increases."],
  "translations": ["对于小样本尺寸和小尺寸，蛮力表现良好。", "数据稀疏性：如果数据稀疏且尺寸较小（<20），KD树的性能将优于Ball Tree算法。", "K的值（邻居）：随着K的增加，KD树和Ball树的查询时间都会增加。"]
}, {
  "tag": "H2",
  "text": "K-D Tree",
  "translation": "K-D树"
}, {
  "tag": "P",
  "text": "To improve the running time, alternate approaches were invented on the line of building a growing tree from point of interest. These methods attempt to reduce the required number of distance calculations by efficiently encoding aggregate distance information for the sample.",
  "translation": "为了提高运行时间，从兴趣点出发，在建造一棵正在生长的树木的线上发明了替代方法。 这些方法试图通过有效地编码样本的集合距离信息来减少所需的距离计算数量。"
}, {
  "tag": "P",
  "text": "The basic idea is that if point A is very distant from point B, and point B is very close to point C, then we know that points A and C are very distant, without having to explicitly calculate their distance. (This may not be correct all the time.)",
  "translation": "基本思想是，如果点A与点B相距很远，而点B与点C相距很近，那么我们知道点A和C相距很远，而不必显式计算它们的距离。 （这可能并非始终都是正确的。）"
}, {
  "tag": "P",
  "text": "In this way, the computational cost of a nearest neighbors search can be reduced to O[D N *log(N)] or better. This is a significant improvement over brute-force for large N.",
  "translation": "这样，可以将最近邻居搜索的计算成本降低到O [D N * log（N）]或更好。 与大N的蛮力相比，这是一个重大改进。"
}, {
  "tag": "P",
  "text": "K-D tree performs well enough when D <20. With larger D it again takes longer time. This is known as “curse of dimensionality”.",
  "translation": "当D <20时，K-D树的性能足够好。 较大的D则需要更长的时间。 这被称为“维数诅咒”。"
}, {
  "tag": "H2",
  "text": "Curse of Dimensionality explained to Kid [Source : StackOverflow]",
  "translation": "向小子解释了维度诅咒[资料来源：StackOverflow]"
}, {
  "tag": "P",
  "text": "Probably the kid will like to eat cookies, so let us assume that you have a whole truck with cookies having a different color, a different shape, a different taste, a different price …",
  "translation": "可能孩子会喜欢吃饼干，所以让我们假设您有一整辆卡车，其中的饼干具有不同的颜色，不同的形状，不同的口味，不同的价格……"
}, {
  "tag": "P",
  "text": "If the kid has to choose but only take into account one characteristic e.g. the taste, then it has four possibilities: sweet, salt, sour, bitter, so the kid only has to try four cookies to find what (s)he likes most.",
  "translation": "如果孩子必须选择，但仅考虑一个特征，例如 口味，那么它就有四种可能性：甜，盐，酸，苦，所以孩子只需要尝试四种饼干就可以找到他最喜欢的东西。"
}, {
  "tag": "P",
  "text": "If the kid likes combinations of taste and colour, and there are 4 (I am rather optimistic here :-) ) different colours, then he already has to choose among 4x4 different types;",
  "translation": "如果孩子喜欢口味和颜色的组合，并且有4种（我对此很乐观：-））不同的颜色，那么他已经必须在4x4的不同类型中进行选择。"
}, {
  "tag": "P",
  "text": "If he wants, in addition, to take into account the shape of the cookies and there are 5 different shapes then he will have to try 4x4x5=80 cookies",
  "translation": "此外，如果他想考虑Cookie的形状并且有5种不同的形状，那么他将不得不尝试4x4x5 = 80个Cookie"
}, {
  "tag": "P",
  "text": "We could go on, but after eating all these cookies he might already have belly-ache … before he can make his best choice :-) Apart from the belly-ache, it can get really difficult to remember the differences in the taste of each cookie.",
  "translation": "我们可以继续，但是在吃完所有这些饼干之后，他可能已经腹痛了……在他做出最佳选择之前：-)除了腹痛，很难记住每种口味的差异 曲奇饼。"
}, {
  "tag": "P",
  "text": "As you can see complexity increases when dimensions/features increases.",
  "translation": "如您所见，随着尺寸/功能的增加，复杂性也会增加。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*OKKOBmrcgnvq5eNCJrFfPA.png?q=20",
  "caption": "When a computer gets virus.",
  "type": "image",
  "file": "1!OKKOBmrcgnvq5eNCJrFfPA.png"
}, {
  "tag": "H1",
  "text": "In Short,",
  "translation": "简而言之，"
}, {
  "tag": "P",
  "text": "An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.",
  "translation": "一个对象按其邻居的多数投票进行分类，该对象被分配给其最接近的k个邻居中最常见的类别（k是一个正整数，通常很小）。 如果k = 1，则仅将对象分配给该单个最近邻居的类。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*rmdr7RsUPOWranwOuuIl7w.png?q=20",
  "caption": "class of star would be B if k =3 and A if k = 6",
  "type": "image",
  "file": "1!rmdr7RsUPOWranwOuuIl7w.png"
}, {
  "tag": "H1",
  "text": "How do we choose neighbors?",
  "translation": "我们如何选择邻居？"
}, {
  "tag": "H2",
  "text": "Brute Force",
  "translation": "蛮力"
}, {
  "tag": "P",
  "text": "Lets consider for simple case with two dimension plot. If we look mathematically, the simple intuition is to calculate the euclidean distance from point of interest ( of whose class we need to determine) to all the points in training set. Then we take class with majority points. This is called brute force method.",
  "translation": "让我们考虑二维绘图的简单情况。 如果我们从数学上看，简单的直觉就是计算从兴趣点（我们需要确定其类别）到训练集中所有点的欧氏距离。 然后我们以多数分数上课。 这称为蛮力法。"
}, {
  "tag": "P",
  "text": "For N samples in D dimensions the running time complexity turns out to be O[DN²]. If we have small number of dimensions and training set, this would run in reasonable time. But as the training set size increases, the running time grows quickly.",
  "translation": "对于D维中的N个样本，运行时间复杂度为O [DN²]。 如果我们的维数和训练集较少，那么这将在合理的时间内运行。 但是，随着训练集大小的增加，运行时间会迅速增加。"
}, {
  "tag": "P",
  "text": "Brute force performs worst when there are large dimensions and large training set.",
  "translation": "当存在较大尺寸和较大训练集时，蛮力表现最差。"
}, {
  "tag": "H1",
  "text": "Chapter 4: K Nearest Neighbors Classifier",
  "translation": "第4章：K最近邻分类器"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*Ug2_0J2UhylXx0Jx6TbRzg.png?q=20",
  "type": "image",
  "file": "1!Ug2_0J2UhylXx0Jx6TbRzg.png"
}, {
  "tag": "P",
  "text": "The fourth and last basic classifier in supervised learning! K nearest Neighbors. In this post, we will discuss about working of K Nearest Neighbors Classifier, the three different underlying algorithms for choosing a neighbor and a part of code snippet for python’s sklearn library. If you haven’t read the previous posts on:",
  "translation": "监督学习中的第四个也是最后一个基本分类器！ K个最近的邻居。 在本文中，我们将讨论K最近邻分类器的工作，三种用于选择邻居的不同基础算法以及python sklearn库的一部分代码片段。 如果您尚未阅读以下文章的前一篇："
}, {
  "tag": "OL",
  "texts": ["Naive Bayes", "SVM (Support Vector)", "Decision Trees"],
  "translations": ["朴素贝叶斯", "SVM（支持向量）", "决策树"]
}, {
  "tag": "P",
  "text": "I would suggest you to go through it prior to this one. (Although this is independent from those three)",
  "translation": "我建议您在此之前先完成它。 （尽管这与那三个独立）"
}, {
  "tag": "PRE",
  "text": "(本文翻译自Savan Patel的文章《Chapter 4: K Nearest Neighbors Classifier》，参考：https://medium.com/machine-learning-101/k-nearest-neighbors-classifier-1c1ff404d265)",
  "translation": "（本文第4章：K最近邻分类器，参考：https：//medium.com/machine-learning-101/k-nearest-neighbors-classifier-1c1ff404d265）"
}]