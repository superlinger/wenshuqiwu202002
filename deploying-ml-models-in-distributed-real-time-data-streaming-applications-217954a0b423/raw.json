[{
  "tag": "H1",
  "text": "Deploying ML Models in Distributed Real-time Data Streaming Applications",
  "translation": "在分布式实时数据流应用程序中部署ML模型"
}, {
  "tag": "H2",
  "text": "Explore the various strategies to deploy ML models in Apache Flink/Spark or other realtime data streaming applications.",
  "translation": "探索在Apache Flink / Spark或其他实时数据流应用程序中部署ML模型的各种策略。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*Lw62ttKzZ87B3ckn?q=20",
  "caption": "Photo by Franck V. on Unsplash",
  "type": "image",
  "file": "0!Lw62ttKzZ87B3ckn"
}, {
  "tag": "P",
  "text": "Machine Learning has gone from zero to one in the past decade. The rise of ML can be seen as one of the most defining moments in the tech industry. Today ML models are ubiquitous in almost all the services.",
  "translation": "在过去的十年中，机器学习已经从零变为一。 ML的兴起可以看作是技术行业中最具决定性的时刻之一。 如今，ML模型在几乎所有服务中无处不在。"
}, {
  "tag": "P",
  "text": "One of the challenges which remain to date is the training and inference of models using real-time data. Let’s take a look at the various strategies which you can use in data streaming production jobs to make predictions.",
  "translation": "迄今为止，仍然存在的挑战之一是使用实时数据进行模型的训练和推理。 让我们看一下您可以在数据流生产作业中使用的各种策略进行预测。"
}, {
  "tag": "H1",
  "text": "Model alongside the pipeline",
  "translation": "与管道一起建模"
}, {
  "tag": "P",
  "text": "The natural approach for predictions on real-time data is to run your ML model in the pipeline processing the data.",
  "translation": "实时数据预测的自然方法是在处理数据的管道中运行ML模型。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*91pmmz85GYMstCMxIPKC_A.jpeg?q=20",
  "caption": "Deploying model in pipeline executors",
  "type": "image",
  "file": "1!91pmmz85GYMstCMxIPKC_A.jpeg"
}, {
  "tag": "P",
  "text": "This approach has two major complications -",
  "translation": "这种方法有两个主要的并发症-"
}, {
  "tag": "OL",
  "texts": ["Integration of the pipeline’s code and the model’s code.", "Optimizing the integrated pipeline to make efficient use of the underlying resources."],
  "translations": ["管道代码和模型代码的集成。", "优化集成管道以有效利用基础资源。"]
}, {
  "tag": "H2",
  "text": "Integration",
  "translation": "积分"
}, {
  "tag": "P",
  "text": "Most of the real-time data pipelines are written using either Java or Python. Both Apache Spark and Apache Flink provide Python API. This allows for easy integration of models written using Scikit-Learn or Tensorflow.",
  "translation": "大多数实时数据管道都是使用Java或Python编写的。 Apache Spark和Apache Flink均提供Python API。 这样可以轻松集成使用Scikit-Learn或Tensorflow编写的模型。"
}, {
  "tag": "P",
  "text": "You can also use Spark MLlib or Flink ML to create models. These models are convenient to integrate, and you don’t have to worry about scaling and fault-tolerance.",
  "translation": "您还可以使用Spark MLlib或Flink ML创建模型。 这些模型易于集成，您不必担心扩展和容错。"
}, {
  "tag": "P",
  "text": "But what if you have a pre-existing data pipeline which is written in Java or Scala? In that case, it makes much more sense to use Tensorflow Java API or third-party libraries such as MLeap or JPMML to export your Scikit-learn models and use them inside your code. JPMML supports a lot of models but MLeap is faster.",
  "translation": "但是，如果您已有一个用Java或Scala编写的数据管道，该怎么办？ 在这种情况下，使用Tensorflow Java API或第三方库（例如MLeap或JPMML）导出Scikit学习模型并在代码中使用它们会更有意义。 JPMML支持许多模型，但是MLeap更快。"
}, {
  "tag": "H2",
  "text": "Optimization",
  "translation": "优化"
}, {
  "tag": "P",
  "text": "The choice between Python and Java/Scala represents a tradeoff between versatility and performance. It would be best if you made the decision based on the use case, the amount of expected data, and the latency expected. I prefer Scala for most of the applications since the expected input records were in the order of millions per second.",
  "translation": "Python和Java / Scala之间的选择代表了多功能性和性能之间的平衡。 最好是根据用例，预期数据量和预期延迟做出决定。 我希望大多数应用程序都使用Scala，因为期望的输入记录约为每秒数百万。"
}, {
  "tag": "P",
  "text": "Another optimization is the number of parallel executors that you should allocate to your model. If it’s a lightweight model such as Logistic Regression or a small Random forest, you can even run a single instance of the model and re-partition data to go to the single executor (this is never a good idea in production). For heavy models such as large random forests or deep neural nets, finding the right number of executors is mostly an exercise in trial & error.",
  "translation": "另一个优化是应分配给模型的并行执行程序的数量。 如果是轻量级模型（例如Logistic回归或小型随机森林），则您甚至可以运行该模型的单个实例，然后将数据重新分区以交给单个执行器（这在生产中绝对不是一个好主意）。 对于诸如大型随机森林或深层神经网络之类的沉重模型，找到合适数量的执行者通常是反复试验的练习。"
}, {
  "tag": "P",
  "text": "You might also need to optimize your ML models so that they can fit in the memory. There are several tools available for this purpose.",
  "translation": "您可能还需要优化ML模型，以使其适合内存。 有多种工具可用于此目的。"
}, {
  "tag": "H2",
  "text": "TensorFlow Lite",
  "translation": "TensorFlow Lite"
}, {
  "tag": "H3",
  "text": "TensorFlow Lite is an open-source deep learning framework for on-device inference.",
  "translation": "TensorFlow Lite是用于设备推断的开源深度学习框架。"
}, {
  "tag": "P",
  "text": "Another complication with this approach is updating the model to a newer version. A fresh deployment is generally required for the update. This also makes A/B testing quite non-trivial.",
  "translation": "这种方法的另一个复杂之处是将模型更新为较新的版本。 通常，此更新需要全新部署。 这也使A / B测试变得相当简单。"
}, {
  "tag": "H1",
  "text": "Model as a REST service",
  "translation": "建模为REST服务"
}, {
  "tag": "P",
  "text": "This is one of the most popular approaches for inference. Run your python code inside a docker container and provide a REST interface to get the results. Tensorflow already provides the REST model serving out of the box.",
  "translation": "这是最流行的推理方法之一。 在docker容器中运行python代码，并提供REST接口以获取结果。 Tensorflow已经提供了现成的REST模型服务。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*QZuaJQQbhaMErv1B1_-x7Q.jpeg?q=20",
  "caption": "Deploying ML Model as a service",
  "type": "image",
  "file": "1!QZuaJQQbhaMErv1B1_-x7Q.jpeg"
}, {
  "tag": "P",
  "text": "For Java, you can use MLeap or DeepLearning4J. You can also dynamically increase/decrease the number of servers according to the throughput in this approach.",
  "translation": "对于Java，您可以使用MLeap或DeepLearning4J。 您还可以根据这种方法根据吞吐量动态增加/减少服务器数量。"
}, {
  "tag": "P",
  "text": "If your model calls are async, this approach fails to trigger back pressure in case there is a burst of data such as during restarts. This can lead to OOM failures in the model servers. Extra precautions must be taken to prevent such scenarios.",
  "translation": "如果您的模型调用是异步的，则这种方法将无法触发背压，以防万一出现数据突发（例如在重新启动期间）。 这可能导致模型服务器中的OOM故障。 必须采取额外的预防措施以防止出现这种情况。"
}, {
  "tag": "P",
  "text": "Latencies are also high since you need a network call to fetch the results. The latencies can slightly be reduced by using gRPC instead of REST.",
  "translation": "延迟也很高，因为您需要网络调用来获取结果。 通过使用gRPC而不是REST可以稍微减少延迟。"
}, {
  "tag": "H2",
  "text": "Here’s How You Can Go Beyond Http 1.1",
  "translation": "这是超越HTTP 1.1的方法"
}, {
  "tag": "H3",
  "text": "Implement your own http/2 service.",
  "translation": "实现您自己的http / 2服务。"
}, {
  "tag": "H1",
  "text": "Database as a model store",
  "translation": "数据库作为模型存储"
}, {
  "tag": "P",
  "text": "If you have a fixed model architecture e.g., Linear Regression, Random Forest, or a small neural net, the weights can be stored in a distributed database such as Cassandra. You can create the model at the runtime using the weights and make the predictions on the new model.",
  "translation": "如果您具有固定的模型体系结构，例如线性回归，随机森林或小型神经网络，则权重可以存储在分布式数据库中，例如Cassandra。 您可以在运行时使用权重创建模型，并对新模型进行预测。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*6dQ664fVvw38WQvf1bQX6g.jpeg?q=20",
  "caption": "Storing models in database",
  "type": "image",
  "file": "1!6dQ664fVvw38WQvf1bQX6g.jpeg"
}, {
  "tag": "P",
  "text": "This approach is a hybrid of the first and second approaches. It allows you to update the model at runtime without requiring a new deployment while also proving back pressure capabilities. It comes at the cost of versatility since you are limiting the number of potential options for the models.",
  "translation": "该方法是第一和第二方法的混合。 它允许您在运行时更新模型，而无需进行新的部署，同时还证明了背压功能。 由于要限制模型的潜在选项数量，因此要牺牲多功能性。"
}, {
  "tag": "H2",
  "text": "So which approach should you choose?",
  "translation": "那么您应该选择哪种方法呢？"
}, {
  "tag": "P",
  "text": "Well, if you want to do a simple POC or your model is pretty lightweight, go with the REST model server. Ease of integration and very few code changes required to run your model makes it an attractive choice. A/B testing can also be done quickly.",
  "translation": "好吧，如果您想做一个简单的POC或您的模型非常轻巧，请使用REST模型服务器。 易于集成，并且运行模型所需的代码更改很少，这使其成为一个有吸引力的选择。 A / B测试也可以快速完成。"
}, {
  "tag": "P",
  "text": "If you require predictions to happen in tens of milliseconds, the pipelined approach is the one to prefer.",
  "translation": "如果您需要在数十毫秒内进行预测，则首选管道方法。"
}, {
  "tag": "P",
  "text": "Lastly, the model store approach should only be used in the case when you have several models e.g., an ML model per city data, and they are lightweight too.",
  "translation": "最后，仅当您有多个模型（例如，每个城市数据的ML模型）并且它们也是轻量型时，才应使用模型存储方法。"
}, {
  "tag": "PRE",
  "text": "(本文翻译自Kartik Khare的文章《Deploying ML Models in Distributed Real-time Data Streaming Applications》，参考：https://towardsdatascience.com/deploying-ml-models-in-distributed-real-time-data-streaming-applications-217954a0b423)",
  "translation": "（本文翻译自Kartik Khare的文章《在分布式实时数据流应用程序中部署ML模型》，参考：https：//towardsdatascience.com/deploying-ml-models-in-distributed-real-time-data-streaming- 应用程序-217954a0b423）"
}]